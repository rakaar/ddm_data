{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from pyvbmc import VBMC\n",
    "import corner\n",
    "from psiam_tied_dv_map_utils import rho_A_t_fn, up_RTs_fit_fn, down_RTs_fit_fn, up_RTs_fit_single_t_fn\n",
    "import sys\n",
    "import multiprocessing\n",
    "from dynesty import NestedSampler\n",
    "from dynesty import plotting as dyplot\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read out_LED.csv as dataframe\n",
    "og_df = pd.read_csv('../out_LED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose non repeat trials - 0 or 2 or missing\n",
    "df = og_df[ og_df['repeat_trial'].isin([0,2]) | og_df['repeat_trial'].isna() ]\n",
    "\n",
    "# only session type 7\n",
    "session_type = 7    \n",
    "df = df[ df['session_type'].isin([session_type]) ]\n",
    "\n",
    "# training level 16\n",
    "training_level = 16\n",
    "df = df[ df['training_level'].isin([training_level]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABL: [20 40 60]\n",
      "ILD: [-16.  -8.  -4.  -2.  -1.   1.   2.   4.   8.  16.]\n"
     ]
    }
   ],
   "source": [
    "# find ABL and ILD\n",
    "ABL_arr = df['ABL'].unique()\n",
    "ILD_arr = df['ILD'].unique()\n",
    "\n",
    "\n",
    "# sort ILD arr in ascending order\n",
    "ILD_arr = np.sort(ILD_arr)\n",
    "ABL_arr = np.sort(ABL_arr)\n",
    "\n",
    "print('ABL:', ABL_arr)\n",
    "print('ILD:', ILD_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LED off rows\n",
    "df_1 = df[ df['LED_trial'] == 0 ]\n",
    "df_1 = df_1[ df_1['timed_fix'] > df_1['intended_fix'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../fitting_aborts/post_led_censor_test_vbmc.pkl', 'rb') as f:\n",
    "    vp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_sample = vp.sample(int(1e6))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_A = np.mean(vp_sample[:,0])\n",
    "theta_A = np.mean(vp_sample[:,1])\n",
    "t_motor = 0.04\n",
    "t_A_aff = np.mean(vp_sample[:,2]) - t_motor\n",
    "# t_A_aff = 0.05 # NOTE: TEMP, to test if negative afferent delay is causing VBMC to not converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.22652285019526627)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_A_aff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VBMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loglike fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loglike(row, rate_lambda, T_0, theta_E, t_E_aff, Z_E, L):\n",
    "    timed_fix = row['timed_fix']\n",
    "    intended_fix = row['intended_fix']\n",
    "    \n",
    "    ILD = row['ILD']\n",
    "    ABL = row['ABL']\n",
    "    choice = row['response_poke']\n",
    "\n",
    "    rt = timed_fix\n",
    "    t_stim = intended_fix\n",
    "    \n",
    "    K_max = 10\n",
    "\n",
    "    if choice == 3:\n",
    "        likelihood = up_RTs_fit_fn([rt], V_A, theta_A, ABL, ILD, rate_lambda, T_0, \\\n",
    "                                    theta_E, Z_E, t_stim, t_A_aff, t_E_aff, t_motor, L, K_max)[0]\n",
    "    elif choice == 2:\n",
    "        likelihood = down_RTs_fit_fn([rt], V_A, theta_A, ABL, ILD, rate_lambda, T_0,\\\n",
    "                                        theta_E, Z_E, t_stim, t_A_aff, t_E_aff, t_motor, L, K_max)[0]\n",
    "\n",
    "\n",
    "    if likelihood <= 0:\n",
    "        likelihood = 1e-50\n",
    "\n",
    "    \n",
    "    return np.log(likelihood)    \n",
    "\n",
    "\n",
    "def psiam_tied_loglike_fn(params):\n",
    "    rate_lambda, T_0, theta_E, t_E_aff, Z_E, L = params\n",
    "\n",
    "\n",
    "    all_loglike = Parallel(n_jobs=30)(delayed(compute_loglike)(row, rate_lambda, T_0, theta_E, t_E_aff, Z_E, L)\\\n",
    "                                       for _, row in df_1.iterrows() if (row['timed_fix'] > row['intended_fix']) \\\n",
    "                                        and (row['response_poke'] in [2,3]))\n",
    "\n",
    "    loglike = np.sum(all_loglike)\n",
    "    return loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dynesty_psiam_tied_loglike_fn(params):\n",
    "    rate_lambda, T_0, theta_E, t_E_aff, Z_E, L = params\n",
    "    \n",
    "    # Filter DataFrame rows first\n",
    "    filtered_rows = df_1.loc[\n",
    "        (df_1['timed_fix'] > df_1['intended_fix']) &\n",
    "        (df_1['response_poke'].isin([2, 3]))\n",
    "    ]\n",
    "    \n",
    "    # Prepare arguments for each row\n",
    "    # We'll map over just the row data. The other parameters are closed over\n",
    "    # from the outer scope.\n",
    "    row_data = [row for _, row in filtered_rows.iterrows()]\n",
    "\n",
    "    # Use multiprocessing Pool\n",
    "    # You can adjust processes=30 to however many processes you need\n",
    "    all_loglike = np.array([compute_loglike(row, rate_lambda, T_0, theta_E, t_E_aff, Z_E, L) for row in row_data])\n",
    "\n",
    "    # Sum the log-likelihoods\n",
    "    loglike = np.sum(all_loglike)\n",
    "\n",
    "    # If loglike is inf or -inf or nan, return log(1e-50)\n",
    "    if np.isnan(loglike) or np.isinf(loglike):\n",
    "        return np.log(1e-50)\n",
    "    else:\n",
    "        return loglike\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_lambda_bounds = [0.01, 0.2]\n",
    "theta_E_bounds = [30, 60]\n",
    "T_0_bounds = [0.1*(1e-3), 1*(1e-3)]\n",
    "\n",
    "\n",
    "t_E_aff_bounds = [0.001, 0.1]\n",
    "Z_E_bounds = [-10, 10]\n",
    "L_bounds = [0.1, 1.99]\n",
    "\n",
    "# ---\n",
    "rate_lambda_plausible_bounds =  [0.05, 0.09]\n",
    "T_0_plausible_bounds = [0.15*(1e-3), 0.5*(1e-3)]\n",
    "theta_E_plausible_bounds = [40, 55]\n",
    "\n",
    "t_E_aff_plausible_bounds = [0.01, 0.05]\n",
    "Z_E_plausible_bounds = [-5, 5]\n",
    "L_plausible_bounds = [0.5, 1.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trapezoidal_logpdf(x, a, b, c, d):\n",
    "    if x < a or x > d:\n",
    "        return -np.inf  # Logarithm of zero\n",
    "    area = ((b - a) + (d - c)) / 2 + (c - b)\n",
    "    h_max = 1.0 / area  # Height of the trapezoid to normalize the area to 1\n",
    "    \n",
    "    if a <= x <= b:\n",
    "        pdf_value = ((x - a) / (b - a)) * h_max\n",
    "    elif b < x < c:\n",
    "        pdf_value = h_max\n",
    "    elif c <= x <= d:\n",
    "        pdf_value = ((d - x) / (d - c)) * h_max\n",
    "    else:\n",
    "        pdf_value = 0.0  # This case is redundant due to the initial check\n",
    "\n",
    "    if pdf_value <= 0.0:\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return np.log(pdf_value)\n",
    "    \n",
    "\n",
    "def psiam_tied_prior_fn(params):\n",
    "    rate_lambda, T_0, theta_E, t_E_aff, Z_E, L = params\n",
    "\n",
    "    rate_lambda_logpdf = trapezoidal_logpdf(rate_lambda, rate_lambda_bounds[0], rate_lambda_plausible_bounds[0], rate_lambda_plausible_bounds[1], rate_lambda_bounds[1])\n",
    "    theta_E_logpdf = trapezoidal_logpdf(theta_E, theta_E_bounds[0], theta_E_plausible_bounds[0], theta_E_plausible_bounds[1], theta_E_bounds[1])\n",
    "    T_0_logpdf = trapezoidal_logpdf(T_0, T_0_bounds[0], T_0_plausible_bounds[0], T_0_plausible_bounds[1], T_0_bounds[1])\n",
    "    \n",
    "    t_E_aff_logpdf = trapezoidal_logpdf(t_E_aff, t_E_aff_bounds[0], t_E_aff_plausible_bounds[0], t_E_aff_plausible_bounds[1], t_E_aff_bounds[1])\n",
    "    Z_E_logpdf = trapezoidal_logpdf(Z_E, Z_E_bounds[0], Z_E_plausible_bounds[0], Z_E_plausible_bounds[1], Z_E_bounds[1])\n",
    "    L_logpdf = trapezoidal_logpdf(L, L_bounds[0], L_plausible_bounds[0], L_plausible_bounds[1], L_bounds[1])\n",
    "\n",
    "    return rate_lambda_logpdf + T_0_logpdf + theta_E_logpdf + t_E_aff_logpdf + Z_E_logpdf + L_logpdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trapezoidal_transform(u, a, b, c, d):\n",
    "    \"\"\"\n",
    "    Maps a uniform random variable u ~ Uniform[0, 1] to a trapezoidal prior distribution.\n",
    "    \"\"\"\n",
    "    area_left = (b - a) / 2\n",
    "    area_middle = c - b\n",
    "    area_right = (d - c) / 2\n",
    "    total_area = area_left + area_middle + area_right\n",
    "\n",
    "    # Normalize u to the total area\n",
    "    u_scaled = u * total_area\n",
    "\n",
    "    if u_scaled <= area_left:\n",
    "        # Left ramp\n",
    "        return a + (2 * u_scaled * (b - a))**0.5\n",
    "    elif u_scaled <= area_left + area_middle:\n",
    "        # Flat top\n",
    "        return b + (u_scaled - area_left)\n",
    "    else:\n",
    "        # Right ramp\n",
    "        return d - (2 * (total_area - u_scaled) * (d - c))**0.5\n",
    "\n",
    "def prior_transform(u):\n",
    "    \"\"\"\n",
    "    Transform a unit cube sample (u ~ Uniform[0,1]) to the parameter space using trapezoidal priors.\n",
    "    \"\"\"\n",
    "    priors = np.zeros_like(u)\n",
    "\n",
    "    # Map each parameter using the trapezoidal transform\n",
    "    priors[0] = trapezoidal_transform(u[0], rate_lambda_bounds[0], rate_lambda_plausible_bounds[0], \n",
    "                                       rate_lambda_plausible_bounds[1], rate_lambda_bounds[1])\n",
    "    priors[1] = trapezoidal_transform(u[1], T_0_bounds[0], T_0_plausible_bounds[0], \n",
    "                                       T_0_plausible_bounds[1], T_0_bounds[1])\n",
    "    priors[2] = trapezoidal_transform(u[2], theta_E_bounds[0], theta_E_plausible_bounds[0], \n",
    "                                       theta_E_plausible_bounds[1], theta_E_bounds[1])\n",
    "    priors[3] = trapezoidal_transform(u[3], t_E_aff_bounds[0], t_E_aff_plausible_bounds[0], \n",
    "                                       t_E_aff_plausible_bounds[1], t_E_aff_bounds[1])\n",
    "    priors[4] = trapezoidal_transform(u[4], Z_E_bounds[0], Z_E_plausible_bounds[0], \n",
    "                                       Z_E_plausible_bounds[1], Z_E_bounds[1])\n",
    "    priors[5] = trapezoidal_transform(u[5], L_bounds[0], L_plausible_bounds[0], \n",
    "                                       L_plausible_bounds[1], L_bounds[1])\n",
    "\n",
    "    return priors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prior + loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psiam_tied_joint_fn(params):\n",
    "    priors = psiam_tied_prior_fn(params) \n",
    "    loglike = psiam_tied_loglike_fn(params)\n",
    "\n",
    "    joint = priors + loglike\n",
    "    return joint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run vbmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = np.array([ rate_lambda_bounds[0], T_0_bounds[0], theta_E_bounds[0], \\\n",
    "               t_E_aff_bounds[0], Z_E_bounds[0], L_bounds[0]])\n",
    "ub = np.array([ rate_lambda_bounds[1], T_0_bounds[1], theta_E_bounds[1], \\\n",
    "                t_E_aff_bounds[1], Z_E_bounds[1], L_bounds[1]])\n",
    "\n",
    "plb = np.array([ rate_lambda_plausible_bounds[0], T_0_plausible_bounds[0], theta_E_plausible_bounds[0], \\\n",
    "                t_E_aff_plausible_bounds[0], Z_E_plausible_bounds[0], L_plausible_bounds[0]])\n",
    "\n",
    "pub = np.array([rate_lambda_plausible_bounds[1], T_0_plausible_bounds[1], theta_E_plausible_bounds[1], \\\n",
    "                t_E_aff_plausible_bounds[1], Z_E_plausible_bounds[1], L_plausible_bounds[1]])\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "rate_lambda_0 = np.random.uniform(rate_lambda_plausible_bounds[0], rate_lambda_plausible_bounds[1])\n",
    "T_0_0 = np.random.uniform(T_0_plausible_bounds[0], T_0_plausible_bounds[1])\n",
    "theta_E_0 = np.random.uniform(theta_E_plausible_bounds[0], theta_E_plausible_bounds[1])\n",
    "\n",
    "t_E_aff_0 = np.random.uniform(t_E_aff_plausible_bounds[0], t_E_aff_plausible_bounds[1])\n",
    "Z_E_0 = np.random.uniform(Z_E_plausible_bounds[0], Z_E_plausible_bounds[1])\n",
    "L_0 = np.random.uniform(L_plausible_bounds[0], L_plausible_bounds[1])\n",
    "\n",
    "x_0 = np.array([rate_lambda_0, T_0_0, theta_E_0, t_E_aff_0, Z_E_0, L_0])\n",
    "\n",
    "vbmc = VBMC(psiam_tied_joint_fn, x_0, lb, ub, plb, pub, options={'display': 'on'})\n",
    "vp, results = vbmc.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbmc.save('NOT_working_ONLY_TIED_params_led_off_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## corner plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_samples = vp.sample(int(1e5))[0]\n",
    "vp_samples[:,1] = vp_samples[:,1] * 1e3\n",
    "\n",
    "param_labels = ['lambda', 'T0', 'theta_E', 't_E', 'Z_E', 'L']\n",
    "\n",
    "percentiles = np.percentile(vp_samples, [0, 100], axis=0)\n",
    "\n",
    "_ranges = [(percentiles[0, i], percentiles[1, i]) for i in np.arange(vp_samples.shape[1])]\n",
    "\n",
    "\n",
    "# Now create the corner plot using these ranges\n",
    "corner.corner(\n",
    "    vp_samples,\n",
    "    labels=param_labels,\n",
    "    show_titles=True,\n",
    "    quantiles=[0.025, 0.5, 0.975],\n",
    "    range=_ranges,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dynesty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m pool \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mN_processes)\n\u001b[1;32m      3\u001b[0m ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m----> 4\u001b[0m sampler \u001b[38;5;241m=\u001b[39m \u001b[43mNestedSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynesty_psiam_tied_loglike_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_processes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m sampler\u001b[38;5;241m.\u001b[39mrun_nested()\n\u001b[1;32m      6\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/raghavendra/code/ddm_data/venv/lib/python3.12/site-packages/dynesty/dynesty.py:680\u001b[0m, in \u001b[0;36mNestedSampler.__new__\u001b[0;34m(cls, loglikelihood, prior_transform, ndim, nlive, bound, sample, periodic, reflective, update_interval, first_update, npdim, rstate, queue_size, pool, use_pool, live_points, logl_args, logl_kwargs, ptform_args, ptform_kwargs, gradient, grad_args, grad_kwargs, compute_jac, enlarge, bootstrap, walks, facc, slices, fmove, max_move, update_func, ncdim, blob, save_history, history_filename)\u001b[0m\n\u001b[1;32m    677\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m grad\n\u001b[1;32m    678\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompute_jac\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m compute_jac\n\u001b[0;32m--> 680\u001b[0m live_points, logvol_init, init_ncalls \u001b[38;5;241m=\u001b[39m \u001b[43m_initialize_live_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloglike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnlive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnlive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_pool_ptform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprior_transform\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;66;03m# Initialize our nested sampler.\u001b[39;00m\n\u001b[1;32m    692\u001b[0m sampler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(_SAMPLERS[bound])\n",
      "File \u001b[0;32m~/raghavendra/code/ddm_data/venv/lib/python3.12/site-packages/dynesty/dynamicsampler.py:438\u001b[0m, in \u001b[0;36m_initialize_live_points\u001b[0;34m(live_points, prior_transform, loglikelihood, M, nlive, ndim, rstate, blob, use_pool_ptform)\u001b[0m\n\u001b[1;32m    436\u001b[0m     cur_live_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(prior_transform, np\u001b[38;5;241m.\u001b[39masarray(cur_live_u))\n\u001b[1;32m    437\u001b[0m cur_live_v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(cur_live_v))\n\u001b[0;32m--> 438\u001b[0m cur_live_logl \u001b[38;5;241m=\u001b[39m \u001b[43mloglikelihood\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_live_v\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blob:\n\u001b[1;32m    440\u001b[0m     cur_live_blobs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([_\u001b[38;5;241m.\u001b[39mblob \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m cur_live_logl])\n",
      "File \u001b[0;32m~/raghavendra/code/ddm_data/venv/lib/python3.12/site-packages/dynesty/utils.py:177\u001b[0m, in \u001b[0;36mLogLikelihood.map\u001b[0;34m(self, pars)\u001b[0m\n\u001b[1;32m    171\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m([\n\u001b[1;32m    172\u001b[0m         LoglOutput(_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblob) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloglikelihood, pars)\n\u001b[1;32m    173\u001b[0m     ])\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     ret \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    176\u001b[0m         LoglOutput(_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblob)\n\u001b[0;32m--> 177\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     ]\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_append([_\u001b[38;5;241m.\u001b[39mval \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m ret], pars)\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_processes = 8\n",
    "pool = multiprocessing.Pool(processes=N_processes)\n",
    "ndim = 6\n",
    "sampler = NestedSampler(dynesty_psiam_tied_loglike_fn, prior_transform, ndim, pool=pool, queue_size=N_processes)\n",
    "sampler.run_nested()\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diagnose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_mean = np.mean(vp_samples[:,0])\n",
    "T0_mean = np.mean(vp_samples[:,1]) / 1e3\n",
    "theta_E_mean = np.mean(vp_samples[:,2])\n",
    "\n",
    "t_E_aff_mean = np.mean(vp_samples[:,3])\n",
    "Z_E_mean = np.mean(vp_samples[:,4])\n",
    "L_mean = np.mean(vp_samples[:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T0_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
