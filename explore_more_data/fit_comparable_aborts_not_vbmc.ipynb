{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported helper functions from time_vary_norm_utils\n",
      "Number of rows where RTwrtStim is NaN and abort_event == 3: 16\n",
      "Shape after removing NaNs in RTwrtStim for abort_event==3: (792588, 62)\n",
      "Shape after filtering for batch and LED trial: (118867, 62)\n",
      "Shape after selecting valid trials and aborts: (90296, 62)\n",
      "Number of abort trials selected: 10598\n",
      "Total trials for fitting (valid + aborts): 90296\n",
      "ABL: [10 25 40 50 55 70]\n",
      "ILD: [-8.   -4.   -2.25 -1.25 -0.5   0.    0.5   1.25  2.25  4.    8.  ]\n",
      "Using Initial Guess (clipped to bounds): [ 0.60544472  2.85673582 -0.16070492]\n",
      "\n",
      "--- Running L-BFGS-B ---\n",
      "Optimizing: MAP (Negative Log Joint)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:590: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- L-BFGS-B Results ---\n",
      "Optimization successful!\n",
      "Found MAP parameters (V_A, theta_A, t_A_aff): [ 0.60544472  2.85673582 -0.16070492]\n",
      "Maximum log posterior value: -520180.585435247\n",
      "Time taken: 7.62 seconds\n",
      "\n",
      "\n",
      "--- Running Differential Evolution ---\n",
      "Optimizing: MAP (Negative Log Joint)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/rlab/raghavendra/ddm_data/.venv/lib/python3.12/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Differential Evolution Results ---\n",
      "Optimization successful!\n",
      "Found MAP parameters (V_A, theta_A, t_A_aff): [ 4.33935744  2.85768139 -0.12724264]\n",
      "Maximum log posterior value: -494352.53566732287\n",
      "Time taken: 11.48 seconds\n",
      "\n",
      "\n",
      "--- Final Optimization Results ---\n",
      "Parameters Format: [V_A, theta_A, t_A_aff]\n",
      "\n",
      "L-BFGS-B MAP Parameters: [ 0.60544472  2.85673582 -0.16070492]\n",
      "\n",
      "Differential Evolution MAP Parameters: [ 4.33935744  2.85768139 -0.12724264]\n",
      "\n",
      "Using Differential Evolution parameters as final result.\n",
      "\n",
      "---> Final Selected Parameters: [ 4.33935744  2.85768139 -0.12724264]\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Didn't converge, better to fit aborts seperately first - SciPy Optimize Version\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "# Comment out or remove pyvbmc import if not used elsewhere\n",
    "# from pyvbmc import VBMC \n",
    "# import corner # Comment out or remove if not used for plotting results here\n",
    "# from tqdm.notebook import tqdm # Use standard tqdm if not in notebook\n",
    "import pickle\n",
    "import random\n",
    "from scipy.integrate import cumulative_trapezoid as cumtrapz\n",
    "import scipy.optimize as opt\n",
    "from scipy.optimize import Bounds\n",
    "from time import time\n",
    "\n",
    "# Assume time_vary_norm_utils is in the python path or same directory\n",
    "try:\n",
    "    from time_vary_norm_utils import (\n",
    "        # up_or_down_RTs_fit_fn, # Not used in this specific script\n",
    "        # cum_pro_and_reactive_time_vary_fn, # Not used\n",
    "        # rho_A_t_VEC_fn, # Not used\n",
    "        # up_or_down_RTs_fit_wrt_stim_fn, # Not used\n",
    "        rho_A_t_fn, \n",
    "        cum_A_t_fn\n",
    "    )\n",
    "    print(\"Successfully imported helper functions from time_vary_norm_utils\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: Could not import helper functions from time_vary_norm_utils.\")\n",
    "    print(\"Please ensure 'time_vary_norm_utils.py' is in the Python path or the same directory.\")\n",
    "    # Define dummy functions if import fails, to allow script to run partially\n",
    "    # THIS IS FOR DEMONSTRATION ONLY - REPLACE WITH ACTUAL FUNCTIONS\n",
    "    def rho_A_t_fn(t, V, theta): return np.exp(-t) if t > 0 else 0\n",
    "    def cum_A_t_fn(t, V, theta): return 1 - np.exp(-t) if t > 0 else 0\n",
    "    print(\"WARNING: Using dummy placeholder functions for rho_A_t_fn and cum_A_t_fn.\")\n",
    "    \n",
    "\n",
    "# Not used in this specific script, comment out or remove\n",
    "# from types import SimpleNamespace\n",
    "# from time_vary_and_norm_simulators import psiam_tied_data_gen_wrapper_rate_norm_fn\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Data Loading and Preprocessing\n",
    "\n",
    "# %%\n",
    "try:\n",
    "    exp_df = pd.read_csv('../outExp.csv')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"Error: '../outExp.csv' not found. Please check the path.\")\n",
    "    \n",
    "    \n",
    "\n",
    "count = ((exp_df['RTwrtStim'].isna()) & (exp_df['abort_event'] == 3)).sum()\n",
    "print(f\"Number of rows where RTwrtStim is NaN and abort_event == 3: {count}\")\n",
    "\n",
    "# Original filtering condition\n",
    "exp_df = exp_df[~((exp_df['RTwrtStim'].isna()) & (exp_df['abort_event'] == 3))].copy()\n",
    "print(f\"Shape after removing NaNs in RTwrtStim for abort_event==3: {exp_df.shape}\")\n",
    "\n",
    "exp_df_batch = exp_df[\n",
    "    (exp_df['batch_name'] == 'Comparable') &\n",
    "    (exp_df['LED_trial'].isin([np.nan, 0]))\n",
    "].copy()\n",
    "print(f\"Shape after filtering for batch and LED trial: {exp_df_batch.shape}\")\n",
    "\n",
    "\n",
    "df_valid_and_aborts = exp_df_batch[\n",
    "    (exp_df_batch['success'].isin([1,-1])) |\n",
    "    (exp_df_batch['abort_event'] == 3)\n",
    "].copy()\n",
    "print(f\"Shape after selecting valid trials and aborts: {df_valid_and_aborts.shape}\")\n",
    "\n",
    "\n",
    "df_aborts = df_valid_and_aborts[df_valid_and_aborts['abort_event'] == 3].copy()\n",
    "\n",
    "# Ensure 'response_poke' and 'ILD' exist before applying functions\n",
    "if 'response_poke' in df_valid_and_aborts.columns:\n",
    "    # 1 is right , -1 is left\n",
    "    df_valid_and_aborts['choice'] = df_valid_and_aborts['response_poke'].apply(lambda x: 1 if x == 3 else (-1 if x == 2 else random.choice([1, -1])))\n",
    "else:\n",
    "    print(\"Warning: 'response_poke' column not found. Skipping 'choice' calculation.\")\n",
    "    df_valid_and_aborts['choice'] = 1 # Assign dummy value\n",
    "\n",
    "if 'ILD' in df_valid_and_aborts.columns and 'choice' in df_valid_and_aborts.columns:\n",
    "    # 1 or 0 if the choice was correct or not\n",
    "    df_valid_and_aborts['accuracy'] = (df_valid_and_aborts['ILD'] * df_valid_and_aborts['choice']).apply(lambda x: 1 if x > 0 else 0)\n",
    "else:\n",
    "     print(\"Warning: 'ILD' or 'choice' column not found. Skipping 'accuracy' calculation.\")\n",
    "     df_valid_and_aborts['accuracy'] = 1 # Assign dummy value\n",
    "\n",
    "df_aborts = df_valid_and_aborts[df_valid_and_aborts['abort_event'] == 3].copy()\n",
    "print(f\"Number of abort trials selected: {df_aborts.shape[0]}\")\n",
    "print(f\"Total trials for fitting (valid + aborts): {df_valid_and_aborts.shape[0]}\")\n",
    "\n",
    "\n",
    "# find ABL and ILD\n",
    "ABL_arr = df_valid_and_aborts['ABL'].unique()\n",
    "ILD_arr = df_valid_and_aborts['ILD'].unique()\n",
    "\n",
    "# sort ILD arr in ascending order\n",
    "ILD_arr = np.sort(ILD_arr)\n",
    "ABL_arr = np.sort(ABL_arr)\n",
    "\n",
    "print('ABL:', ABL_arr)\n",
    "print('ILD:', ILD_arr)\n",
    "\n",
    "if df_valid_and_aborts.empty:\n",
    "    raise SystemExit(\"ERROR: No data left after filtering. Check data loading and filtering steps.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Model Parameters and Constants\n",
    "\n",
    "# %%\n",
    "proactive_trunc_time = 0.3\n",
    "N_JOBS = 30 # Number of parallel jobs for likelihood calculation\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Log-Likelihood Function\n",
    "\n",
    "# %%\n",
    "def compute_loglike(row, V_A, theta_A, t_A_aff):\n",
    "    \"\"\"Calculates the log-likelihood for a single trial row.\"\"\"\n",
    "    \n",
    "    # Use RTwrtStim if it's a valid trial (success==1 or -1), otherwise use TotalFixTime for aborts\n",
    "    # This assumes RTwrtStim is the relevant time for the likelihood calculation of successful trials,\n",
    "    # and TotalFixTime is relevant for aborts. Adjust if this logic is incorrect.\n",
    "    \n",
    "    # IMPORTANT: Check if TotalFixTime or intended_fix makes sense for your model.\n",
    "    # The original code used TotalFixTime and intended_fix. Reverting to that.\n",
    "    timed_fix = row['TotalFixTime']\n",
    "    intended_fix = row['intended_fix']\n",
    "    \n",
    "    rt = timed_fix       # Use TotalFixTime as the observed time\n",
    "    t_stim = intended_fix # Use intended_fix as the stimulus/censoring time\n",
    "\n",
    "    if pd.isna(rt) or pd.isna(t_stim):\n",
    "        # Handle potential missing values if necessary\n",
    "        # print(f\"Warning: NaN found in row: {row.name}, rt={rt}, t_stim={t_stim}. Skipping row.\")\n",
    "        return 0 # Or handle appropriately, e.g., raise error or impute\n",
    "        \n",
    "    try:\n",
    "        # Calculate cumulative probability up to proactive_trunc_time (used for truncation)\n",
    "        cum_prob_trunc = cum_A_t_fn(proactive_trunc_time - t_A_aff, V_A, theta_A)\n",
    "        trunc_factor = 1.0 - cum_prob_trunc\n",
    "        \n",
    "        # Add small epsilon to avoid division by zero if trunc_factor is exactly 0\n",
    "        trunc_factor = max(trunc_factor, 1e-50) \n",
    "\n",
    "        if row['abort_event'] == 3: # Abort trial\n",
    "            # Density only non-zero if abort time is >= proactive_trunc_time\n",
    "            if rt < proactive_trunc_time:\n",
    "                pdf = 0.0 \n",
    "            else:\n",
    "                pdf = rho_A_t_fn(rt - t_A_aff, V_A, theta_A)\n",
    "        else: # Valid trial (success == 1 or -1)\n",
    "            # Probability of *not* aborting before stimulus time t_stim\n",
    "            pdf = 1.0 - cum_A_t_fn(t_stim - t_A_aff, V_A, theta_A)\n",
    "\n",
    "        # Normalize by the truncation factor (probability of not aborting before proactive_trunc_time)\n",
    "        pdf /= trunc_factor\n",
    "        \n",
    "        # Floor the pdf to avoid log(0)\n",
    "        pdf = max(pdf, 1e-50) \n",
    "\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"Error during PDF/CDF calculation for row {row.name}: {e}\")\n",
    "        print(f\"Params: V_A={V_A}, theta_A={theta_A}, t_A_aff={t_A_aff}\")\n",
    "        print(f\"Inputs: rt={rt}, t_stim={t_stim}, proactive_trunc_time={proactive_trunc_time}\")\n",
    "        # Decide how to handle: return large negative loglike, 0, or raise error\n",
    "        return -np.inf \n",
    "\n",
    "    log_pdf = np.log(pdf)\n",
    "\n",
    "    if np.isnan(log_pdf):\n",
    "        # print(f'Warning: NaN log_pdf encountered.')\n",
    "        # print(f'row[\"abort_event\"] = {row[\"abort_event\"]}')\n",
    "        # print(f'row[\"TotalFixTime\"] / rt = {rt}')\n",
    "        # print(f'row[\"intended_fix\"] / t_stim = {t_stim}')\n",
    "        # print(f'Calculated pdf = {pdf}')\n",
    "        # print(f'Params: V_A={V_A}, theta_A={theta_A}, t_A_aff={t_A_aff}')\n",
    "        # print(f'Trunc Factor = {trunc_factor}')\n",
    "        return -np.inf # Return large negative value for invalid log-likelihood\n",
    "\n",
    "    return log_pdf\n",
    "\n",
    "\n",
    "def aggregate_loglike_fn(params):\n",
    "    \"\"\"Aggregates log-likelihood over all trials using joblib.\"\"\"\n",
    "    V_A, theta_A, t_A_aff = params\n",
    "    \n",
    "    # Check for obviously bad parameter values early\n",
    "    if V_A <= 0 or theta_A <= 0:\n",
    "        return -np.inf\n",
    "\n",
    "    try:\n",
    "        all_loglike = Parallel(n_jobs=N_JOBS)(delayed(compute_loglike)(row, V_A, theta_A, t_A_aff)\n",
    "                                           for _, row in df_aborts.iterrows())\n",
    "        \n",
    "        # Check if any results are None or non-numeric before summing\n",
    "        valid_loglikes = [ll for ll in all_loglike if isinstance(ll, (int, float)) and np.isfinite(ll)]\n",
    "        \n",
    "        if not valid_loglikes:\n",
    "             print(\"Warning: No valid log-likelihoods computed.\")\n",
    "             return -np.inf\n",
    "\n",
    "        total_loglike = np.sum(valid_loglikes)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during parallel computation with params {params}: {e}\")\n",
    "        total_loglike = -np.inf # Assign large negative value on error\n",
    "\n",
    "    # Add a check for the final sum being non-finite\n",
    "    if not np.isfinite(total_loglike):\n",
    "        # print(f\"Warning: Non-finite total log-likelihood ({total_loglike}) for params {params}\")\n",
    "        return -np.inf\n",
    "        \n",
    "    return total_loglike\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Prior Function\n",
    "\n",
    "# %%\n",
    "def trapezoidal_logpdf(x, a, b, c, d):\n",
    "    \"\"\"Computes the log PDF of a trapezoidal distribution.\"\"\"\n",
    "    if x < a or x > d or a > b or b > c or c > d:\n",
    "        return -np.inf  # Outside support or invalid bounds\n",
    "\n",
    "    # Avoid division by zero if boundaries coincide\n",
    "    width_bottom = d - a\n",
    "    width_top = c - b\n",
    "    if width_bottom <= 0: return -np.inf # Invalid base\n",
    "    \n",
    "    area = (width_bottom + width_top) / 2.0\n",
    "    if area <= 0: return -np.inf # Should not happen with valid a,b,c,d\n",
    "\n",
    "    h_max = 1.0 / area  # Height of the normalized trapezoid\n",
    "\n",
    "    if a <= x < b:\n",
    "        # Handle vertical slope case b=a\n",
    "        slope_ab = h_max / (b - a) if (b - a) > 1e-12 else np.inf\n",
    "        pdf_value = (x - a) * slope_ab\n",
    "    elif b <= x <= c:\n",
    "        pdf_value = h_max\n",
    "    elif c < x <= d:\n",
    "        # Handle vertical slope case d=c\n",
    "        slope_cd = h_max / (d - c) if (d - c) > 1e-12 else np.inf\n",
    "        pdf_value = (d - x) * slope_cd\n",
    "    else: \n",
    "         # This case included for completeness, already handled by initial check\n",
    "        pdf_value = 0.0\n",
    "\n",
    "    if pdf_value <= 1e-100: # Check against small positive number for log\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return np.log(pdf_value)\n",
    "\n",
    "\n",
    "# Define bounds (copied from the original script)\n",
    "V_A_bounds = [0.01, 10]\n",
    "theta_A_bounds = [0.1, 6]\n",
    "t_A_aff_bounds = [-1, 0.1]\n",
    "\n",
    "V_A_plausible_bounds = [0.1, 3]\n",
    "theta_A_plausible_bounds = [0.5, 4]\n",
    "t_A_aff_plausible_bounds = [-0.25, 0.05]\n",
    "\n",
    "plb = np.array([\n",
    "    V_A_plausible_bounds[0],\n",
    "    theta_A_plausible_bounds[0],\n",
    "    t_A_aff_plausible_bounds[0]\n",
    "])\n",
    "\n",
    "pub = np.array([\n",
    "    V_A_plausible_bounds[1],\n",
    "    theta_A_plausible_bounds[1],\n",
    "    t_A_aff_plausible_bounds[1]\n",
    "])\n",
    "\n",
    "def prior_fn(params):\n",
    "    \"\"\"Computes the joint log-prior for the parameters.\"\"\"\n",
    "    V_A, theta_A, t_A_aff = params\n",
    "\n",
    "    V_A_logpdf = trapezoidal_logpdf(\n",
    "        V_A,\n",
    "        V_A_bounds[0], V_A_plausible_bounds[0], V_A_plausible_bounds[1], V_A_bounds[1]\n",
    "    )\n",
    "    \n",
    "    theta_A_logpdf = trapezoidal_logpdf(\n",
    "        theta_A,\n",
    "        theta_A_bounds[0], theta_A_plausible_bounds[0], theta_A_plausible_bounds[1], theta_A_bounds[1]\n",
    "    )\n",
    "    \n",
    "    t_A_aff_logpdf = trapezoidal_logpdf(\n",
    "        t_A_aff,\n",
    "        t_A_aff_bounds[0], t_A_aff_plausible_bounds[0], t_A_aff_plausible_bounds[1], t_A_aff_bounds[1]\n",
    "    )\n",
    "    \n",
    "    # Check for -inf which indicates parameters are outside prior support\n",
    "    if np.isneginf(V_A_logpdf) or np.isneginf(theta_A_logpdf) or np.isneginf(t_A_aff_logpdf):\n",
    "        return -np.inf\n",
    "\n",
    "    return V_A_logpdf + theta_A_logpdf + t_A_aff_logpdf\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Joint Log-Posterior Function\n",
    "\n",
    "# %%\n",
    "def joint_log_posterior_fn(params):\n",
    "    \"\"\"Computes the joint log-posterior (log-prior + log-likelihood).\"\"\"\n",
    "    log_prior = prior_fn(params)\n",
    "    \n",
    "    # If prior is -inf, parameters are invalid, no need to calculate likelihood\n",
    "    if np.isneginf(log_prior):\n",
    "        return -np.inf\n",
    "        \n",
    "    log_likelihood = aggregate_loglike_fn(params)\n",
    "    \n",
    "    # If likelihood calculation failed or resulted in -inf\n",
    "    if np.isneginf(log_likelihood):\n",
    "         return -np.inf\n",
    "\n",
    "    return log_prior + log_likelihood\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Objective Functions for Minimization (Negative Log-Posterior/Likelihood)\n",
    "\n",
    "# %%\n",
    "# Define hard bounds (lower and upper)\n",
    "lb = np.array([V_A_bounds[0], theta_A_bounds[0], t_A_aff_bounds[0]])\n",
    "ub = np.array([V_A_bounds[1], theta_A_bounds[1], t_A_aff_bounds[1]])\n",
    "bounds_obj = Bounds(lb, ub)\n",
    "\n",
    "# Bounds for differential_evolution (list of tuples)\n",
    "bounds_list = list(zip(lb, ub))\n",
    "\n",
    "# Objective function for MAP estimation (minimize negative log-posterior)\n",
    "def neg_log_joint(params):\n",
    "    # First check hard bounds - essential for some optimizers\n",
    "    if not np.all((params >= lb) & (params <= ub)):\n",
    "        return np.inf # Use np.inf for minimization objective outside bounds\n",
    "        \n",
    "    log_posterior = joint_log_posterior_fn(params)\n",
    "    \n",
    "    # Handle -inf return (e.g., params outside prior support or likelihood error)\n",
    "    if np.isneginf(log_posterior):\n",
    "        return np.inf # Return a large positive number for minimization\n",
    "    \n",
    "    return -log_posterior\n",
    "\n",
    "# Objective function for MLE estimation (minimize negative log-likelihood)\n",
    "# Use this if you want to ignore the prior\n",
    "def neg_log_likelihood(params):\n",
    "    # First check hard bounds\n",
    "    if not np.all((params >= lb) & (params <= ub)):\n",
    "        return np.inf \n",
    "        \n",
    "    log_like = aggregate_loglike_fn(params)\n",
    "    \n",
    "    # Handle -inf return\n",
    "    if np.isneginf(log_like):\n",
    "        return np.inf\n",
    "        \n",
    "    return -log_like\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Setup for Optimization\n",
    "\n",
    "# %%\n",
    "# Bounds for L-BFGS-B (using scipy.optimize.Bounds)\n",
    "bounds_obj = Bounds(lb, ub)\n",
    "\n",
    "# Bounds for differential_evolution (list of tuples)\n",
    "bounds_list = list(zip(lb, ub))\n",
    "\n",
    "# Initial Guess (using plausible centers or provided values)\n",
    "# x_0 = np.array([\n",
    "#     (V_A_plausible_bounds[0] + V_A_plausible_bounds[1]) / 2,\n",
    "#     (theta_A_plausible_bounds[0] + theta_A_plausible_bounds[1]) / 2,\n",
    "#     (t_A_aff_plausible_bounds[0] + t_A_aff_plausible_bounds[1]) / 2,\n",
    "# ])\n",
    "# Use the user's initial guess\n",
    "x_0 = np.clip(np.random.uniform(low=plb, high=pub, size=len(plb)), lb, ub)\n",
    "\n",
    "# Ensure x_0 is within hard bounds\n",
    "# x_0 = np.clip(x_0, lb, ub) \n",
    "print(f\"Using Initial Guess (clipped to bounds): {x_0}\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Run Optimization Methods\n",
    "\n",
    "# %%\n",
    "\n",
    "# --- Method 1: L-BFGS-B (Local Optimization for MAP) ---\n",
    "print(\"\\n--- Running L-BFGS-B ---\")\n",
    "objective_fn_lbfgsb = neg_log_joint # Use neg_log_joint for MAP\n",
    "print(f\"Optimizing: MAP (Negative Log Joint)\")\n",
    "\n",
    "start_time_lbfgsb = time()\n",
    "result_lbfgsb = None # Initialize result variable\n",
    "try:\n",
    "    result_lbfgsb = opt.minimize(\n",
    "        objective_fn_lbfgsb,\n",
    "        x_0,\n",
    "        method='L-BFGS-B',\n",
    "        bounds=bounds_obj,\n",
    "        options={'disp': False, 'maxiter': 200, 'ftol': 1e-7, 'gtol': 1e-5} # Standard options\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"L-BFGS-B optimization failed with error: {e}\")\n",
    "    \n",
    "end_time_lbfgsb = time()\n",
    "\n",
    "print(\"\\n--- L-BFGS-B Results ---\")\n",
    "params_lbfgsb = None\n",
    "if result_lbfgsb is not None and result_lbfgsb.success:\n",
    "    print(f\"Optimization successful!\")\n",
    "    params_lbfgsb = result_lbfgsb.x\n",
    "    max_log_posterior_lbfgsb = -result_lbfgsb.fun \n",
    "    print(f\"Found MAP parameters (V_A, theta_A, t_A_aff): {params_lbfgsb}\")\n",
    "    print(f\"Maximum log posterior value: {max_log_posterior_lbfgsb}\")\n",
    "else:\n",
    "    status_msg = result_lbfgsb.message if result_lbfgsb is not None else \"Optimization did not run or failed.\"\n",
    "    print(f\"Optimization failed: {status_msg}\")\n",
    "    if result_lbfgsb is not None:\n",
    "         print(f\"Last parameters: {result_lbfgsb.x}\")\n",
    "         print(f\"Last function value: {-result_lbfgsb.fun}\")\n",
    "\n",
    "print(f\"Time taken: {end_time_lbfgsb - start_time_lbfgsb:.2f} seconds\")\n",
    "\n",
    "\n",
    "# --- Method 2: Differential Evolution (Global Optimization for MAP) ---\n",
    "print(\"\\n\\n--- Running Differential Evolution ---\")\n",
    "objective_fn_de = neg_log_joint # Use neg_log_joint for MAP\n",
    "print(f\"Optimizing: MAP (Negative Log Joint)\")\n",
    "\n",
    "# Adjust workers based on your joblib N_JOBS and system cores\n",
    "# Using N_JOBS directly assumes DE can efficiently manage these workers\n",
    "n_workers_de = N_JOBS # Use the same number of workers as likelihood calculation\n",
    "\n",
    "start_time_de = time()\n",
    "result_de = None # Initialize result variable\n",
    "try:\n",
    "    result_de = opt.differential_evolution(\n",
    "        objective_fn_de,\n",
    "        bounds=bounds_list,\n",
    "        strategy='best1bin',    # Common strategy\n",
    "        maxiter=100,            # Iterations limit (adjust as needed)\n",
    "        popsize=15,             # Population size (rule of thumb: >= 5*Dim)\n",
    "        tol=0.01,               # Convergence tolerance\n",
    "        mutation=(0.5, 1),      # Mutation factor range\n",
    "        recombination=0.7,      # Recombination probability\n",
    "        disp=False,              # Display progress\n",
    "        workers=n_workers_de,   # Use parallel workers\n",
    "        updating='deferred'     # Recommended when using workers\n",
    "    )\n",
    "except Exception as e:\n",
    "     print(f\"Differential Evolution optimization failed with error: {e}\")\n",
    "     \n",
    "end_time_de = time()\n",
    "\n",
    "print(\"\\n--- Differential Evolution Results ---\")\n",
    "params_de = None\n",
    "if result_de is not None and result_de.success:\n",
    "    print(f\"Optimization successful!\")\n",
    "    params_de = result_de.x\n",
    "    max_log_posterior_de = -result_de.fun \n",
    "    print(f\"Found MAP parameters (V_A, theta_A, t_A_aff): {params_de}\")\n",
    "    print(f\"Maximum log posterior value: {max_log_posterior_de}\")\n",
    "else:\n",
    "    # DE might terminate without success flag but still have a result\n",
    "    if result_de is not None and hasattr(result_de, 'x'):\n",
    "         status_msg = result_de.message if hasattr(result_de, 'message') else \"Terminated without success flag.\"\n",
    "         print(f\"Optimization terminated: {status_msg}\")\n",
    "         params_de = result_de.x # Still store the best found params\n",
    "         print(f\"Best parameters found: {params_de}\")\n",
    "         print(f\"Best log posterior value found: {-result_de.fun}\")\n",
    "    else:\n",
    "         print(f\"Optimization did not run or failed catastrophically.\")\n",
    "\n",
    "\n",
    "print(f\"Time taken: {end_time_de - start_time_de:.2f} seconds\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Final Parameter Output\n",
    "\n",
    "# %%\n",
    "print(\"\\n\\n--- Final Optimization Results ---\")\n",
    "\n",
    "print(\"Parameters Format: [V_A, theta_A, t_A_aff]\")\n",
    "\n",
    "if params_lbfgsb is not None:\n",
    "    print(f\"\\nL-BFGS-B MAP Parameters: {params_lbfgsb}\")\n",
    "    # You could re-evaluate the likelihood/posterior at this point if needed\n",
    "    # final_log_post_lbfgsb = joint_log_posterior_fn(params_lbfgsb)\n",
    "    # print(f\"L-BFGS-B Final Log Posterior: {final_log_post_lbfgsb}\")\n",
    "else:\n",
    "    print(\"\\nL-BFGS-B did not converge successfully or did not run.\")\n",
    "\n",
    "if params_de is not None:\n",
    "     print(f\"\\nDifferential Evolution MAP Parameters: {params_de}\")\n",
    "     # final_log_post_de = joint_log_posterior_fn(params_de)\n",
    "     # print(f\"Differential Evolution Final Log Posterior: {final_log_post_de}\")\n",
    "else:\n",
    "    print(\"\\nDifferential Evolution did not converge successfully or did not run.\")\n",
    "\n",
    "# You can choose which parameters to use going forward, e.g., from DE if it found a better value\n",
    "best_params = None\n",
    "if params_de is not None:\n",
    "    best_params = params_de\n",
    "    print(f\"\\nUsing Differential Evolution parameters as final result.\")\n",
    "elif params_lbfgsb is not None:\n",
    "    best_params = params_lbfgsb\n",
    "    print(f\"\\nUsing L-BFGS-B parameters as final result (DE failed).\")\n",
    "else:\n",
    "    print(\"\\nNo successful optimization result obtained.\")\n",
    "\n",
    "if best_params is not None:\n",
    "    print(f\"\\n---> Final Selected Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Didn't converge, better to fit aborts seperately first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from pyvbmc import VBMC\n",
    "import corner\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import random\n",
    "from scipy.integrate import cumulative_trapezoid as cumtrapz\n",
    "\n",
    "from time_vary_norm_utils import (\n",
    "    up_or_down_RTs_fit_fn, cum_pro_and_reactive_time_vary_fn,\n",
    "    rho_A_t_VEC_fn, up_or_down_RTs_fit_wrt_stim_fn, rho_A_t_fn, cum_A_t_fn)\n",
    "from types import SimpleNamespace\n",
    "from time_vary_and_norm_simulators import psiam_tied_data_gen_wrapper_rate_norm_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = pd.read_csv('../outExp.csv')\n",
    "\n",
    "count = ((exp_df['RTwrtStim'].isna()) & (exp_df['abort_event'] == 3)).sum()\n",
    "print(\"Number of rows where RTwrtStim is NaN and abort_event == 3:\", count)\n",
    "\n",
    "\n",
    "exp_df = exp_df[~((exp_df['RTwrtStim'].isna()) & (exp_df['abort_event'] == 3))].copy()\n",
    "\n",
    "exp_df_batch = exp_df[\n",
    "    (exp_df['batch_name'] == 'Comparable') &\n",
    "    (exp_df['LED_trial'].isin([np.nan, 0]))\n",
    "]\n",
    "\n",
    "df_valid_and_aborts = exp_df_batch[\n",
    "    (exp_df_batch['success'].isin([1,-1])) |\n",
    "    (exp_df_batch['abort_event'] == 3)\n",
    "].copy()\n",
    "\n",
    "# 1 is right , -1 is left\n",
    "df_valid_and_aborts['choice'] = df_valid_and_aborts['response_poke'].apply(lambda x: 1 if x == 3 else (-1 if x == 2 else random.choice([1, -1])))\n",
    "\n",
    "# 1 or 0 if the choice was correct or not\n",
    "df_valid_and_aborts['accuracy'] = (df_valid_and_aborts['ILD'] * df_valid_and_aborts['choice']).apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "df_aborts = df_valid_and_aborts[df_valid_and_aborts['abort_event'] == 3]\n",
    "# find ABL and ILD\n",
    "ABL_arr = df_valid_and_aborts['ABL'].unique()\n",
    "ILD_arr = df_valid_and_aborts['ILD'].unique()\n",
    "\n",
    "\n",
    "# sort ILD arr in ascending order\n",
    "ILD_arr = np.sort(ILD_arr)\n",
    "ABL_arr = np.sort(ABL_arr)\n",
    "\n",
    "print('ABL:', ABL_arr)\n",
    "print('ILD:', ILD_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vbmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_norm = False\n",
    "is_time_vary = False\n",
    "phi_params_obj = np.nan\n",
    "rate_norm_l = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proactive_trunc_time = 0.3\n",
    "K_max = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loglike fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loglike(row, V_A, theta_A, t_A_aff):\n",
    "    \n",
    "    timed_fix = row['TotalFixTime']\n",
    "    intended_fix = row['intended_fix']\n",
    "    \n",
    "    \n",
    "    rt = timed_fix\n",
    "    t_stim = intended_fix\n",
    "    trunc_factor = 1 - cum_A_t_fn(proactive_trunc_time - t_A_aff, V_A, theta_A)\n",
    "\n",
    "    if row['abort_event'] == 3:\n",
    "        if rt < proactive_trunc_time:\n",
    "            pdf = 0\n",
    "        else:\n",
    "            pdf = rho_A_t_fn(rt - t_A_aff, V_A, theta_A)\n",
    "    else:\n",
    "        pdf = 1 - cum_A_t_fn(t_stim - t_A_aff, V_A, theta_A)\n",
    "\n",
    "    pdf /= (trunc_factor + 1e-50)\n",
    "    pdf = max(pdf, 1e-50)\n",
    "\n",
    "    if np.isnan(pdf):\n",
    "        print(f'row[\"abort_event\"] = {row[\"abort_event\"]}')\n",
    "        print(f'row[\"RTwrtStim\"] = {row[\"RTwrtStim\"]}')\n",
    "        raise ValueError(f'nan pdf rt = {rt}, t_stim = {t_stim}')\n",
    "    \n",
    "    return np.log(pdf)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def vbmc_loglike_fn(params):\n",
    "    V_A, theta_A, t_A_aff = params\n",
    "    all_loglike = Parallel(n_jobs=30)(delayed(compute_loglike)(row, V_A, theta_A, t_A_aff)\\\n",
    "                                    #    for _, row in df_aborts.iterrows() )\n",
    "                                       for _, row in df_valid_and_aborts.iterrows() )\n",
    "\n",
    "    return np.sum(all_loglike)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_A_bounds = [0.01, 10]\n",
    "theta_A_bounds = [0.1, 6]\n",
    "t_A_aff_bounds = [-1, 0.1]\n",
    "\n",
    "\n",
    "V_A_plausible_bounds = [0.1, 3]\n",
    "theta_A_plausible_bounds = [0.5, 4]\n",
    "t_A_aff_plausible_bounds = [-0.25, 0.05]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trapezoidal_logpdf(x, a, b, c, d):\n",
    "    if x < a or x > d:\n",
    "        return -np.inf  # Logarithm of zero\n",
    "    area = ((b - a) + (d - c)) / 2 + (c - b)\n",
    "    h_max = 1.0 / area  # Height of the trapezoid to normalize the area to 1\n",
    "    \n",
    "    if a <= x <= b:\n",
    "        pdf_value = ((x - a) / (b - a)) * h_max\n",
    "    elif b < x < c:\n",
    "        pdf_value = h_max\n",
    "    elif c <= x <= d:\n",
    "        pdf_value = ((d - x) / (d - c)) * h_max\n",
    "    else:\n",
    "        pdf_value = 0.0  # This case is redundant due to the initial check\n",
    "\n",
    "    if pdf_value <= 0.0:\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return np.log(pdf_value)\n",
    "    \n",
    "\n",
    "def vbmc_prior_fn(params):\n",
    "    V_A, theta_A, t_A_aff = params\n",
    "\n",
    "    V_A_logpdf = trapezoidal_logpdf(\n",
    "        V_A,\n",
    "        V_A_bounds[0],\n",
    "        V_A_plausible_bounds[0],\n",
    "        V_A_plausible_bounds[1],\n",
    "        V_A_bounds[1]\n",
    "    )\n",
    "    \n",
    "    theta_A_logpdf = trapezoidal_logpdf(\n",
    "        theta_A,\n",
    "        theta_A_bounds[0],\n",
    "        theta_A_plausible_bounds[0],\n",
    "        theta_A_plausible_bounds[1],\n",
    "        theta_A_bounds[1]\n",
    "    )\n",
    "    \n",
    "    t_A_aff_logpdf = trapezoidal_logpdf(\n",
    "        t_A_aff,\n",
    "        t_A_aff_bounds[0],\n",
    "        t_A_aff_plausible_bounds[0],\n",
    "        t_A_aff_plausible_bounds[1],\n",
    "        t_A_aff_bounds[1]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (\n",
    "        V_A_logpdf +\n",
    "        theta_A_logpdf +\n",
    "        t_A_aff_logpdf\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prior + loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vbmc_joint_fn(params):\n",
    "    priors = vbmc_prior_fn(params)\n",
    "    loglike = vbmc_loglike_fn(params)\n",
    "\n",
    "    return priors + loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run vbmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bounds for all parameters (order: V_A, theta_A, t_A_aff, rate_lambda, T_0, theta_E, w, t_E_aff, del_go)\n",
    "lb = np.array([\n",
    "    V_A_bounds[0],\n",
    "    theta_A_bounds[0],\n",
    "    t_A_aff_bounds[0],\n",
    "])\n",
    "\n",
    "ub = np.array([\n",
    "    V_A_bounds[1],\n",
    "    theta_A_bounds[1],\n",
    "    t_A_aff_bounds[1]\n",
    "])\n",
    "\n",
    "plb = np.array([\n",
    "    V_A_plausible_bounds[0],\n",
    "    theta_A_plausible_bounds[0],\n",
    "    t_A_aff_plausible_bounds[0]\n",
    "])\n",
    "\n",
    "pub = np.array([\n",
    "    V_A_plausible_bounds[1],\n",
    "    theta_A_plausible_bounds[1],\n",
    "    t_A_aff_plausible_bounds[1]\n",
    "])\n",
    "\n",
    "# Initialize with random values within plausible bounds\n",
    "np.random.seed(42)\n",
    "# V_A_0 = np.random.uniform(*V_A_plausible_bounds)\n",
    "# theta_A_0 = np.random.uniform(*theta_A_plausible_bounds)\n",
    "# t_A_aff_0 = np.random.uniform(*t_A_aff_plausible_bounds)\n",
    "\n",
    "V_A_0 = 2.8\n",
    "theta_A_0 = 3.2\n",
    "t_A_aff_0 = -0.22\n",
    "\n",
    "x_0 = np.array([\n",
    "    V_A_0,\n",
    "    theta_A_0,\n",
    "    t_A_aff_0\n",
    "])\n",
    "\n",
    "# Run VBMC\n",
    "vbmc = VBMC(vbmc_joint_fn, x_0, lb, ub, plb, pub, options={'display': 'on'})\n",
    "vp, results = vbmc.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vbmc.save('ONLY_norm_vbmc_fit.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the VBMC posterior (returns tuple: samples, log weights)\n",
    "vp_samples = vp.sample(int(1e5))[0]\n",
    "\n",
    "# Convert T_0 to ms (T_0 is at index 4)\n",
    "\n",
    "# Parameter labels (order matters!)\n",
    "param_labels = [\n",
    "    r'$V_A$',           # 0\n",
    "    r'$\\theta_A$',      # 1\n",
    "    r'$t_A^{aff}$',     # 2\n",
    "]\n",
    "\n",
    "# Compute 1st and 99th percentiles for each param to restrict range\n",
    "percentiles = np.percentile(vp_samples, [1, 99], axis=0)\n",
    "_ranges = [(percentiles[0, i], percentiles[1, i]) for i in range(vp_samples.shape[1])]\n",
    "\n",
    "# Create the corner plot\n",
    "fig = corner.corner(\n",
    "    vp_samples,\n",
    "    labels=param_labels,\n",
    "    show_titles=True,\n",
    "    quantiles=[0.025, 0.5, 0.975],\n",
    "    range=_ranges,\n",
    "    title_fmt=\".3f\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_A = vp_samples[:,0].mean()\n",
    "theta_A = vp_samples[:,1].mean()\n",
    "t_A_aff = vp_samples[:,2].mean()\n",
    "\n",
    "\n",
    "print(f'V_A: {V_A}')\n",
    "print(f'theta_A: {theta_A}')\n",
    "print(f't_A_aff: {t_A_aff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "N_theory = int(1e3)\n",
    "t_pts = np.arange(0,1.25, 0.001)\n",
    "t_stim_samples = df_valid_and_aborts.sample(N_theory)['intended_fix']\n",
    "pdf_samples = np.zeros((N_theory, len(t_pts)))\n",
    "\n",
    "for i, t_stim in enumerate(t_stim_samples):\n",
    "    t_stim_idx = np.searchsorted(t_pts, t_stim)\n",
    "    proactive_trunc_idx = np.searchsorted(t_pts, proactive_trunc_time)\n",
    "    pdf_samples[i, :proactive_trunc_idx] = 0\n",
    "    pdf_samples[i, t_stim_idx:] = 0\n",
    "    t_btn = t_pts[proactive_trunc_idx:t_stim_idx-1]\n",
    "    \n",
    "    pdf_samples[i, proactive_trunc_idx:t_stim_idx-1] = rho_A_t_VEC_fn(t_btn - t_A_aff, V_A, theta_A) / (1 - cum_A_t_fn(proactive_trunc_time - t_A_aff, V_A, theta_A))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "bins = np.arange(0,2,0.02)\n",
    "df_aborts = df_valid_and_aborts[df_valid_and_aborts['abort_event'] == 3]\n",
    "df_aborts_RT = df_aborts['TotalFixTime'].dropna().values\n",
    "df_aborts_RT_trunc = df_aborts_RT[df_aborts_RT > proactive_trunc_time]\n",
    "\n",
    "frac_aborts = len(df_aborts_RT_trunc) / len(df_valid_and_aborts)\n",
    "aborts_hist, _ = np.histogram(df_aborts_RT_trunc, bins=bins, density=True)\n",
    "\n",
    "plt.plot(bins[:-1], aborts_hist * frac_aborts, label='data')\n",
    "plt.xlabel('abort rt')\n",
    "plt.ylabel('density')\n",
    "\n",
    "plt.plot(t_pts, np.mean(pdf_samples, axis=0), label='theory')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vbmc.save('non_linear_only_norm_decent_non_convg.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_lambda = vp_samples[:, 0].mean()\n",
    "T_0 = vp_samples[:, 1].mean()\n",
    "theta_E = vp_samples[:, 2].mean()\n",
    "w = vp_samples[:, 3].mean()\n",
    "Z_E = (w - 0.5) * 2 * theta_E\n",
    "t_E_aff = vp_samples[:, 4].mean()\n",
    "rate_norm_l = vp_samples[:, 5].mean()\n",
    "\n",
    "# Print them out\n",
    "print(\"Posterior Means:\")\n",
    "print(f\"rate_lambda  = {rate_lambda:.5f}\")\n",
    "print(f\"T_0 (ms)      = {1e3*T_0:.5f}\")\n",
    "print(f\"theta_E       = {theta_E:.5f}\")\n",
    "print(f\"Z_E           = {Z_E:.5f}\")\n",
    "print(f\"t_E_aff       = {1e3*t_E_aff:.5f} ms\")\n",
    "print(f\"rate_norm_l   = {rate_norm_l:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample t-stim\n",
    "N_sim = int(1e6)\n",
    "\n",
    "t_stim_samples = df_led_off['intended_fix'].sample(N_sim, replace=True).values\n",
    "ABL_samples = df_led_off['ABL'].sample(N_sim, replace=True).values\n",
    "ILD_samples = df_led_off['ILD'].sample(N_sim, replace=True).values\n",
    "\n",
    "N_print = int(N_sim / 5)\n",
    "dt  = 1e-4\n",
    "\n",
    "sim_results = Parallel(n_jobs=30)(\n",
    "    delayed(psiam_tied_data_gen_wrapper_rate_norm_fn)(\n",
    "        V_A, theta_A, ABL_samples[iter_num], ILD_samples[iter_num], rate_lambda, T_0, theta_E, Z_E, t_A_aff, t_E_aff, del_go, \n",
    "        t_stim_samples[iter_num], rate_norm_l, iter_num, N_print, dt\n",
    "    ) for iter_num in tqdm(range(N_sim))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ******* TEMP add back ILD 16 *****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LED off rows\n",
    "df_led_off = df[ df['LED_trial'] == 0 ]\n",
    "print(f'len df_led_off = {len(df_led_off)}')\n",
    "\n",
    "# > 0 and < 1s valid rt \n",
    "df_led_off_valid = df_led_off[\n",
    "    (df_led_off['timed_fix'] - df_led_off['intended_fix'] > 0) &\n",
    "    (df_led_off['timed_fix'] - df_led_off['intended_fix'] < 1)\n",
    "]\n",
    "\n",
    "df_led_off_valid = df_led_off_valid[df_led_off_valid['response_poke'].isin([2,3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare valid sim df, data df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_results_df = pd.DataFrame(sim_results)\n",
    "sim_results_df_valid = sim_results_df[\n",
    "    (sim_results_df['rt'] > sim_results_df['t_stim']) &\n",
    "    (sim_results_df['rt'] - sim_results_df['t_stim'] < 1)\n",
    "].copy()\n",
    "sim_results_df_valid.loc[:, 'correct'] = (sim_results_df_valid['ILD'] * sim_results_df_valid['choice']).apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "\n",
    "df_led_off_valid_renamed = df_led_off_valid.rename(columns = {\n",
    "    'timed_fix': 'rt',\n",
    "    'intended_fix': 't_stim'\n",
    "}).copy()\n",
    "\n",
    "sim_df_1 = sim_results_df_valid.copy()\n",
    "data_df_1 = df_led_off_valid_renamed.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = 0.02\n",
    "bins = np.arange(0, 1, bw)\n",
    "bin_centers = bins[:-1] + (0.5 * bw)\n",
    "\n",
    "n_rows = len(ILD_arr)\n",
    "n_cols = len(ABL_arr)\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 3 * n_rows), sharey='row')\n",
    "\n",
    "for i_idx, ILD in enumerate(ILD_arr):\n",
    "    for a_idx, ABL in enumerate(ABL_arr):\n",
    "        ax = axs[i_idx, a_idx] if n_rows > 1 else axs[a_idx]\n",
    "\n",
    "        sim_df_1_ABL_ILD = sim_df_1[(sim_df_1['ABL'] == ABL) & (sim_df_1['ILD'] == ILD)]\n",
    "        data_df_1_ABL_ILD = data_df_1[(data_df_1['ABL'] == ABL) & (data_df_1['ILD'] == ILD)]\n",
    "\n",
    "        sim_up = sim_df_1_ABL_ILD[sim_df_1_ABL_ILD['choice'] == 1]\n",
    "        sim_down = sim_df_1_ABL_ILD[sim_df_1_ABL_ILD['choice'] == -1]\n",
    "        data_up = data_df_1_ABL_ILD[data_df_1_ABL_ILD['choice'] == 1]\n",
    "        data_down = data_df_1_ABL_ILD[data_df_1_ABL_ILD['choice'] == -1]\n",
    "\n",
    "        sim_up_rt = sim_up['rt'] - sim_up['t_stim']\n",
    "        sim_down_rt = sim_down['rt'] - sim_down['t_stim']\n",
    "        data_up_rt = data_up['rt'] - data_up['t_stim']\n",
    "        data_down_rt = data_down['rt'] - data_down['t_stim']\n",
    "\n",
    "        sim_up_hist, _ = np.histogram(sim_up_rt, bins=bins, density=True)\n",
    "        sim_down_hist, _ = np.histogram(sim_down_rt, bins=bins, density=True)\n",
    "        data_up_hist, _ = np.histogram(data_up_rt, bins=bins, density=True)\n",
    "        data_down_hist, _ = np.histogram(data_down_rt, bins=bins, density=True)\n",
    "\n",
    "        # Normalize histograms by proportion of trials\n",
    "        sim_up_hist *= len(sim_up) / len(sim_df_1_ABL_ILD)\n",
    "        sim_down_hist *= len(sim_down) / len(sim_df_1_ABL_ILD)\n",
    "        data_up_hist *= len(data_up) / len(data_df_1_ABL_ILD)\n",
    "        data_down_hist *= len(data_down) / len(data_df_1_ABL_ILD)\n",
    "\n",
    "        # Plot\n",
    "        ax.plot(bin_centers, data_up_hist, color='b', label='Data' if (i_idx == 0 and a_idx == 0) else \"\")\n",
    "        ax.plot(bin_centers, -data_down_hist, color='b')\n",
    "        ax.plot(bin_centers, sim_up_hist, color='r', label='Sim' if (i_idx == 0 and a_idx == 0) else \"\")\n",
    "        ax.plot(bin_centers, -sim_down_hist, color='r')\n",
    "\n",
    "        # Compute fractions\n",
    "        data_total = len(data_df_1_ABL_ILD)\n",
    "        sim_total = len(sim_df_1_ABL_ILD)\n",
    "        data_up_frac = len(data_up) / data_total if data_total else 0\n",
    "        data_down_frac = len(data_down) / data_total if data_total else 0\n",
    "        sim_up_frac = len(sim_up) / sim_total if sim_total else 0\n",
    "        sim_down_frac = len(sim_down) / sim_total if sim_total else 0\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"ABL: {ABL}, ILD: {ILD}\\n\"\n",
    "            f\"Data,Sim: (+{data_up_frac:.2f},+{sim_up_frac:.2f}), \"\n",
    "            f\"(-{data_down_frac:.2f},-{sim_down_frac:.2f})\"\n",
    "        )\n",
    "        \n",
    "        ax.axhline(0, color='k', linewidth=0.5)\n",
    "        ax.set_xlim([0, 0.7])\n",
    "        if a_idx == 0:\n",
    "            ax.set_ylabel(\"Density (Up / Down flipped)\")\n",
    "        if i_idx == n_rows - 1:\n",
    "            ax.set_xlabel(\"RT (s)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = 0.02\n",
    "bins = np.arange(0, 1, bw)\n",
    "bin_centers = bins[:-1] + (0.5 * bw)\n",
    "\n",
    "def plot_tacho(df_1):\n",
    "    df_1 = df_1.copy()\n",
    "    df_1['RT_bin'] = pd.cut(df_1['rt'] - df_1['t_stim'], bins=bins, include_lowest=True)\n",
    "    grouped = df_1.groupby('RT_bin', observed=False)['correct'].agg(['mean', 'count'])\n",
    "    grouped['bin_mid'] = grouped.index.map(lambda x: x.mid)\n",
    "    return grouped['bin_mid'], grouped['mean']\n",
    "\n",
    "n_rows = len(ILD_arr)\n",
    "n_cols = len(ABL_arr)\n",
    "\n",
    "# === Define fig2 ===\n",
    "fig2, axs = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 3 * n_rows), sharey='row')\n",
    "\n",
    "for i_idx, ILD in enumerate(ILD_arr):\n",
    "    for a_idx, ABL in enumerate(ABL_arr):\n",
    "        ax = axs[i_idx, a_idx] if n_rows > 1 else axs[a_idx]\n",
    "\n",
    "        sim_df_1_ABL_ILD = sim_df_1[(sim_df_1['ABL'] == ABL) & (sim_df_1['ILD'] == ILD)]\n",
    "        data_df_1_ABL_ILD = data_df_1[(data_df_1['ABL'] == ABL) & (data_df_1['ILD'] == ILD)]\n",
    "\n",
    "        sim_tacho_x, sim_tacho_y = plot_tacho(sim_df_1_ABL_ILD)\n",
    "        data_tacho_x, data_tacho_y = plot_tacho(data_df_1_ABL_ILD)\n",
    "\n",
    "        # Plotting\n",
    "        ax.plot(data_tacho_x, data_tacho_y, color='b', label='Data' if (i_idx == 0 and a_idx == 0) else \"\")\n",
    "        ax.plot(sim_tacho_x, sim_tacho_y, color='r', label='Sim' if (i_idx == 0 and a_idx == 0) else \"\")\n",
    "\n",
    "        ax.set_ylim([0.5, 1.05])\n",
    "        ax.set_xlim([0, 0.7])\n",
    "        ax.set_title(f\"ABL: {ABL}, ILD: {ILD}\")\n",
    "        if a_idx == 0:\n",
    "            ax.set_ylabel(\"P(correct)\")\n",
    "        if i_idx == n_rows - 1:\n",
    "            ax.set_xlabel(\"RT (s)\")\n",
    "\n",
    "fig2.tight_layout()\n",
    "fig2.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grand RTDs and tacho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grand_rtd(df_1):\n",
    "    df_1_rt = df_1['rt'] - df_1['t_stim']\n",
    "    rt_hist, _ = np.histogram(df_1_rt, bins=bins, density=True)\n",
    "    return rt_hist\n",
    "\n",
    "def plot_psycho(df_1):\n",
    "    prob_choice_dict = {}\n",
    "\n",
    "    all_ABL = np.sort(df_1['ABL'].unique())\n",
    "    all_ILD = np.sort(df_1['ILD'].unique())\n",
    "\n",
    "    for abl in all_ABL:\n",
    "        filtered_df = df_1[df_1['ABL'] == abl]\n",
    "        prob_choice_dict[abl] = [\n",
    "            sum(filtered_df[filtered_df['ILD'] == ild]['choice'] == 1) / len(filtered_df[filtered_df['ILD'] == ild])\n",
    "            for ild in all_ILD\n",
    "        ]\n",
    "\n",
    "    return prob_choice_dict\n",
    "\n",
    "# === Define fig3 ===\n",
    "fig3, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# === Grand RTD ===\n",
    "axes[0].plot(bin_centers, grand_rtd(data_df_1), color='b', label='data')\n",
    "axes[0].plot(bin_centers, grand_rtd(sim_df_1), color='r', label='sim')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('rt wrt stim')\n",
    "axes[0].set_ylabel('density')\n",
    "axes[0].set_title('Grand RTD')\n",
    "\n",
    "# === Grand Psychometric ===\n",
    "data_psycho = plot_psycho(data_df_1)\n",
    "sim_psycho = plot_psycho(sim_df_1)\n",
    "\n",
    "colors = ['r', 'b', 'g']  # Adjust colors for your ABLs\n",
    "for i, ABL in enumerate(ABL_arr):\n",
    "    axes[1].plot(ILD_arr, data_psycho[ABL], color=colors[i], label=f'data ABL={ABL}', marker='o', linestyle='None')\n",
    "    axes[1].plot(ILD_arr, sim_psycho[ABL], color=colors[i], linestyle='-')\n",
    "\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('ILD')\n",
    "axes[1].set_ylabel('P(right)')\n",
    "axes[1].set_title('Grand Psychometric')\n",
    "\n",
    "# === Grand Tacho ===\n",
    "data_tacho_x, data_tacho_y = plot_tacho(data_df_1)\n",
    "sim_tacho_x, sim_tacho_y = plot_tacho(sim_df_1)\n",
    "\n",
    "axes[2].plot(data_tacho_x, data_tacho_y, color='b', label='data')\n",
    "axes[2].plot(sim_tacho_x, sim_tacho_y, color='r', label='sim')\n",
    "axes[2].legend()\n",
    "axes[2].set_xlabel('rt wrt stim')\n",
    "axes[2].set_ylabel('acc')\n",
    "axes[2].set_title('Grand Tacho')\n",
    "axes[2].set_ylim(0.5, 1)\n",
    "\n",
    "fig3.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all in a single PDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "# Set your filename prefix\n",
    "output_filename = 'no_ILD_16_V4_NON_LINEAR_ONLY_Norm_report'\n",
    "\n",
    "# Ensure output directory\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# === Save individual figures as PNGs ===\n",
    "fig1_path = f'outputs/{output_filename}_updown_hist.png'\n",
    "fig2_path = f'outputs/{output_filename}_tacho.png'\n",
    "fig3_path = f'outputs/{output_filename}_grand_summary.png'\n",
    "\n",
    "fig.savefig(fig1_path)\n",
    "fig2.savefig(fig2_path)\n",
    "fig3.savefig(fig3_path)\n",
    "\n",
    "# === Create PDF with all three figures ===\n",
    "pdf_path = f'outputs/{output_filename}.pdf'\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "    for fig_item in [fig, fig2, fig3]:\n",
    "        pdf.savefig(fig_item)\n",
    "\n",
    "# === Create DOCX with all three figures ===\n",
    "doc = Document()\n",
    "doc.add_heading('RTD and Tacho Analysis Results', 0)\n",
    "\n",
    "for img_path in [fig1_path, fig2_path, fig3_path]:\n",
    "    doc.add_page_break()\n",
    "    doc.add_picture(img_path, width=Inches(6.5))\n",
    "\n",
    "docx_path = f'outputs/{output_filename}.docx'\n",
    "doc.save(docx_path)\n",
    "\n",
    "print(f\" Saved PDF to: {pdf_path}\")\n",
    "print(f\" Saved DOCX to: {docx_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# up and down RTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_theory = int(1e3)\n",
    "t_stim_and_led_tuple = [(row['intended_fix'], row['intended_fix'] - row['LED_onset_time']) for _, row in df.iterrows()]\n",
    "random_indices = np.random.randint(0, len(t_stim_and_led_tuple), N_theory)\n",
    "t_pts = np.arange(-1, 2, 0.001)\n",
    "\n",
    "P_A_samples = np.zeros((N_theory, len(t_pts)))\n",
    "for idx in range(N_theory):\n",
    "    t_stim, t_LED = t_stim_and_led_tuple[random_indices[idx]]\n",
    "    pdf = rho_A_t_VEC_fn(t_pts - t_A_aff + t_stim, V_A, theta_A)\n",
    "    P_A_samples[idx, :] = pdf\n",
    "\n",
    "P_A_samples_mean = np.mean(P_A_samples, axis=0)\n",
    "C_A_mean = cumtrapz(P_A_samples_mean, t_pts, initial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes row by row to enable row-wise shared Y axes\n",
    "fig = plt.figure(figsize=(18, 24))\n",
    "axes = []\n",
    "\n",
    "for i in range(10):  # 10 rows\n",
    "    row_axes = []\n",
    "    for j in range(3):  # 3 columns\n",
    "        ax = fig.add_subplot(10, 3, i * 3 + j + 1, sharey=row_axes[0] if row_axes else None)\n",
    "        row_axes.append(ax)\n",
    "    axes.append(row_axes)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.8, wspace=0.4)\n",
    "\n",
    "bin_size = 0.02\n",
    "bins = np.arange(-1, 2, bin_size)\n",
    "bin_centers = bins[:-1] + (bin_size / 2)\n",
    "t_pts = np.arange(-1, 2, 0.001)\n",
    "\n",
    "phi_params_obj = np.nan\n",
    "\n",
    "for a_idx, ABL in enumerate(ABL_arr):\n",
    "    for i_idx, ILD in enumerate(ILD_arr):\n",
    "        ax = axes[i_idx][a_idx]  # axes[row][col] = (ILD, ABL)\n",
    "\n",
    "        # data\n",
    "        df_led_off_abort_and_valid = df_led_off[(df_led_off['abort_event'] == 3) | (df_led_off['response_poke'].isin([2,3]))]\n",
    "        mask_invalid = ~df_led_off_abort_and_valid['response_poke'].isin([2, 3])\n",
    "        # Step 2: Assign random values (2 or 3 with 50% chance) to those rows\n",
    "        df_led_off_abort_and_valid.loc[mask_invalid, 'response_poke'] = np.random.choice([2, 3], size=mask_invalid.sum())\n",
    "\n",
    "        df_ABL_ILD = df_led_off_abort_and_valid[\n",
    "            (df_led_off_abort_and_valid['ABL'] == ABL) & (df_led_off_abort_and_valid['ILD'] == ILD)]\n",
    "        # df_ABL_ILD = df_led_off_valid[\n",
    "        #     (df_led_off_valid['ABL'] == ABL) & (df_led_off_valid['ILD'] == ILD)]\n",
    "        df_ABL_ILD_up = df_ABL_ILD[df_ABL_ILD['response_poke'] == 3]\n",
    "        df_ABL_ILD_down = df_ABL_ILD[df_ABL_ILD['response_poke'] == 2]\n",
    "\n",
    "        df_ABL_ILD_up_rt = df_ABL_ILD_up['timed_fix'] - df_ABL_ILD_up['intended_fix']\n",
    "        df_ABL_ILD_down_rt = df_ABL_ILD_down['timed_fix'] - df_ABL_ILD_down['intended_fix']\n",
    "\n",
    "        data_up_rt_hist, _ = np.histogram(df_ABL_ILD_up_rt, bins=bins, density=True)\n",
    "        data_down_rt_hist, _ = np.histogram(df_ABL_ILD_down_rt, bins=bins, density=True)\n",
    "\n",
    "        data_frac_up = len(df_ABL_ILD_up) / len(df_ABL_ILD)\n",
    "        data_frac_down = len(df_ABL_ILD_down) / len(df_ABL_ILD)\n",
    "\n",
    "        # theory\n",
    "        theory_ABL_ILD_up = np.zeros_like(t_pts)\n",
    "        theory_ABL_ILD_down = np.zeros_like(t_pts)\n",
    "\n",
    "        for idx, t in enumerate(t_pts):\n",
    "            P_A = P_A_samples_mean[idx]\n",
    "            C_A = C_A_mean[idx]\n",
    "            theory_ABL_ILD_up[idx] = up_or_down_RTs_fit_wrt_stim_fn(\n",
    "                t, 1,\n",
    "                P_A, C_A,\n",
    "                t_stim, ABL, ILD, rate_lambda, T_0, theta_E, Z_E, t_E_aff, del_go,\n",
    "                phi_params_obj, rate_norm_l,\n",
    "                is_norm, is_time_vary, K_max)\n",
    "\n",
    "            theory_ABL_ILD_down[idx] = up_or_down_RTs_fit_wrt_stim_fn(\n",
    "                t, -1,\n",
    "                P_A, C_A,\n",
    "                t_stim, ABL, ILD, rate_lambda, T_0, theta_E, Z_E, t_E_aff, del_go,\n",
    "                phi_params_obj, rate_norm_l,\n",
    "                is_norm, is_time_vary, K_max)\n",
    "\n",
    "        ax.plot(bin_centers, data_up_rt_hist * data_frac_up, 'b--', label='Data Up' if i_idx == 0 and a_idx == 0 else \"\")\n",
    "        ax.plot(bin_centers, -data_down_rt_hist * data_frac_down, 'b--', label='Data Down' if i_idx == 0 and a_idx == 0 else \"\")\n",
    "        ax.plot(t_pts, theory_ABL_ILD_up, 'r-', label='Theory Up' if i_idx == 0 and a_idx == 0 else \"\")\n",
    "        ax.plot(t_pts, -theory_ABL_ILD_down, 'r-', label='Theory Down' if i_idx == 0 and a_idx == 0 else \"\")\n",
    "\n",
    "        ax.set_title(f'ABL={ABL}, ILD={ILD}', fontsize=9)\n",
    "        ax.axhline(0, color='black', linewidth=0.5)\n",
    "        ax.set_xlim(-0.2, 0.7)\n",
    "\n",
    "# Add single legend outside the plot\n",
    "handles, labels = axes[0][0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.98, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tachometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the figure and axes\n",
    "fig, axes = plt.subplots(nrows=10, ncols=3, figsize=(18, 24), sharex=True, sharey=True)\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "bin_size = 0.02\n",
    "bins = np.arange(-1, 2, bin_size)\n",
    "bin_centers = bins[:-1] + (bin_size / 2)\n",
    "t_pts = np.arange(-1, 2, 0.001)\n",
    "\n",
    "phi_params_obj = np.nan\n",
    "\n",
    "for a_idx, ABL in enumerate(ABL_arr):\n",
    "    for i_idx, ILD in enumerate(ILD_arr):\n",
    "        ax = axes[i_idx, a_idx]  # (row=ILD, col=ABL)\n",
    "\n",
    "        # data\n",
    "        df_led_off_abort_and_valid = df_led_off[(df_led_off['abort_event'] == 3) | (df_led_off['response_poke'].isin([2,3]))]\n",
    "        mask_invalid = ~df_led_off_abort_and_valid['response_poke'].isin([2, 3])\n",
    "        df_led_off_abort_and_valid.loc[mask_invalid, 'response_poke'] = np.random.choice([2, 3], size=mask_invalid.sum())\n",
    "\n",
    "        df_ABL_ILD = df_led_off_abort_and_valid[\n",
    "            (df_led_off_abort_and_valid['ABL'] == ABL) & (df_led_off_abort_and_valid['ILD'] == ILD)].copy()\n",
    "        # df_ABL_ILD = df_led_off_valid[\n",
    "        #     (df_led_off_valid['ABL'] == ABL) & (df_led_off_valid['ILD'] == ILD)].copy()\n",
    "        \n",
    "        df_ABL_ILD.loc[:, 'RT'] = df_ABL_ILD['timed_fix'] - df_ABL_ILD['intended_fix']\n",
    "        df_ABL_ILD.loc[:, 'is_correct'] = (\n",
    "                    df_ABL_ILD['ILD'] * (2 * df_ABL_ILD['response_poke'] - 5)\n",
    "                ) > 0\n",
    "        df_ABL_ILD.loc[:, 'rt_bin'] = pd.cut(\n",
    "                df_ABL_ILD['RT'], bins=bins, include_lowest=True\n",
    "            )\n",
    "        tachometric_curve = df_ABL_ILD.groupby('rt_bin', observed=False)['is_correct'].mean()\n",
    "\n",
    "\n",
    "        # theory\n",
    "        theory_ABL_ILD_up = np.zeros_like(t_pts)\n",
    "        theory_ABL_ILD_down = np.zeros_like(t_pts)\n",
    "        theory_tacho = np.zeros_like(t_pts)\n",
    "        for idx, t in enumerate(t_pts):\n",
    "            P_A = P_A_samples_mean[idx]\n",
    "            C_A = C_A_mean[idx]\n",
    "            theory_ABL_ILD_up[idx] = up_or_down_RTs_fit_wrt_stim_fn(\n",
    "                t, 1,\n",
    "                P_A, C_A,\n",
    "                t_stim, ABL, ILD, rate_lambda, T_0, theta_E, Z_E, t_E_aff, del_go,\n",
    "                phi_params_obj, rate_norm_l,\n",
    "                is_norm, is_time_vary, K_max)\n",
    "\n",
    "            theory_ABL_ILD_down[idx] = up_or_down_RTs_fit_wrt_stim_fn(\n",
    "                t, -1,\n",
    "                P_A, C_A,\n",
    "                t_stim, ABL, ILD, rate_lambda, T_0, theta_E, Z_E, t_E_aff, del_go,\n",
    "                phi_params_obj, rate_norm_l,\n",
    "                is_norm, is_time_vary, K_max)\n",
    "\n",
    "            if ILD > 0:\n",
    "                theory_tacho[idx] = theory_ABL_ILD_up[idx] / (theory_ABL_ILD_up[idx] + theory_ABL_ILD_down[idx] + 1e-10)\n",
    "            else:\n",
    "                theory_tacho[idx] = theory_ABL_ILD_down[idx] / (theory_ABL_ILD_up[idx] + theory_ABL_ILD_down[idx] + 1e-10)\n",
    "        \n",
    "\n",
    "        ax.plot(bin_centers, tachometric_curve, 'b--')\n",
    "        ax.plot(t_pts, theory_tacho, 'r-')\n",
    "        ax.set_title(f'ABL={ABL}, ILD={ILD}', fontsize=9)\n",
    "        ax.axhline(0, color='black', linewidth=0.5)\n",
    "        ax.set_xlim(0, 0.7)\n",
    "        ax.set_ylim(0.5, 1.05)\n",
    "\n",
    "# Add shared axis labels\n",
    "\n",
    "# Add a single legend outside the plot\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.98, 1])  # leave space for legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grand rtd, psycho, tacho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample t-stim\n",
    "N_sim = int(1e6)\n",
    "\n",
    "t_stim_samples = df['intended_fix'].sample(N_sim, replace=True).values\n",
    "ABL_samples = df['ABL'].sample(N_sim, replace=True).values\n",
    "ILD_samples = df['ILD'].sample(N_sim, replace=True).values\n",
    "\n",
    "N_print = int(N_sim / 5)\n",
    "dt  = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_results = Parallel(n_jobs=30)(\n",
    "    delayed(psiam_tied_data_gen_wrapper_rate_norm_fn)(\n",
    "        V_A, theta_A, ABL_samples[iter_num], ILD_samples[iter_num], rate_lambda, T_0, theta_E, Z_E, t_A_aff, t_E_aff, del_go, \n",
    "        t_stim_samples[iter_num], rate_norm_l, iter_num, N_print, dt\n",
    "    ) for iter_num in tqdm(range(N_sim))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim df\n",
    "sim_results_df = pd.DataFrame(sim_results)\n",
    "sim_results_df_valid = sim_results_df[sim_results_df['rt'] > sim_results_df['t_stim']]\n",
    "sim_results_df_valid_less_than_1 = sim_results_df_valid[sim_results_df_valid['rt'] - sim_results_df_valid['t_stim'] < 1].copy()\n",
    "\n",
    "# rename data df cols\n",
    "df_led_off_valid.loc[:,'choice'] = 2*df_led_off_valid['response_poke'] - 5\n",
    "df_led_off_valid_renamed = df_led_off_valid.rename(columns={\n",
    "    'timed_fix': 'rt',\n",
    "    'intended_fix': 't_stim'\n",
    "}).copy()\n",
    "\n",
    "# add corr cols in both\n",
    "df_led_off_valid_renamed.loc[:,'correct'] = (df_led_off_valid_renamed['choice'] * df_led_off_valid_renamed['ILD'] > 0).astype(int)\n",
    "sim_results_df_valid_less_than_1.loc[:,'correct'] = (sim_results_df_valid_less_than_1['choice'] * sim_results_df_valid_less_than_1['ILD'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = 0.02\n",
    "bins = np.arange(0, 1, bw)\n",
    "bin_centers = bins[:-1] + 0.5*bw\n",
    "def grand_rtd(df_1):\n",
    "    df_1_rt = df_1['rt'] - df_1['t_stim']\n",
    "    rt_hist, _ = np.histogram(df_1_rt, bins=bins, density=True)\n",
    "    return rt_hist\n",
    "\n",
    "def plot_psycho(df_1):\n",
    "    prob_choice_dict = {}\n",
    "\n",
    "    all_ABL = np.sort(df_1['ABL'].unique())\n",
    "    all_ILD = np.sort(df_1['ILD'].unique())\n",
    "\n",
    "    for abl in all_ABL:\n",
    "        filtered_df = df_1[df_1['ABL'] == abl]\n",
    "        prob_choice_dict[abl] = [sum(filtered_df[filtered_df['ILD'] == ild]['choice'] == 1) / len(filtered_df[filtered_df['ILD'] == ild]) for ild in all_ILD]\n",
    "\n",
    "    return prob_choice_dict\n",
    "\n",
    "def plot_tacho(df_1):\n",
    "    # prob of correct vs RT\n",
    "    df_1.loc[:, 'RT_bin'] = pd.cut(df_1['rt'] - df_1['t_stim'], bins=bins, include_lowest=True)\n",
    "    grouped_by_rt_bin = df_1.groupby('RT_bin', observed=False)['correct'].agg(['mean', 'count'])\n",
    "    grouped_by_rt_bin['bin_mid'] = grouped_by_rt_bin.index.map(lambda x: x.mid)\n",
    "    return grouped_by_rt_bin['bin_mid'], grouped_by_rt_bin['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with 3 subplots in a single row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# === grand RTD ===\n",
    "axes[0].plot(bin_centers, grand_rtd(df_led_off_valid_renamed), color='b', label='data')\n",
    "axes[0].plot(bin_centers, grand_rtd(sim_results_df_valid_less_than_1), color='r', label='sim')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('rt wrt stim')\n",
    "axes[0].set_ylabel('density')\n",
    "axes[0].set_title('Grand RTD')\n",
    "\n",
    "# === grand psycho ===\n",
    "data_psycho = plot_psycho(df_led_off_valid_renamed)\n",
    "sim_psycho = plot_psycho(sim_results_df_valid_less_than_1)\n",
    "\n",
    "colors = ['r', 'b', 'g']  # Define colors for each ABL\n",
    "for i, ABL in enumerate(ABL_arr):\n",
    "    axes[1].plot(ILD_arr, data_psycho[ABL], color=colors[i], label=f'data ABL={ABL}', marker='o', linestyle='None')\n",
    "    axes[1].plot(ILD_arr, sim_psycho[ABL], color=colors[i], label=f'sim ABL={ABL}', linestyle='-')\n",
    "\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('ILD')\n",
    "axes[1].set_ylabel('P(right)')\n",
    "axes[1].set_title('Grand Psychometric')\n",
    "\n",
    "# === grand tacho ===\n",
    "data_tacho_x, data_tacho_y = plot_tacho(df_led_off_valid_renamed)\n",
    "sim_tacho_x, sim_tacho_y = plot_tacho(sim_results_df_valid_less_than_1)\n",
    "\n",
    "axes[2].plot(data_tacho_x, data_tacho_y, color='b', label='data')\n",
    "axes[2].plot(sim_tacho_x, sim_tacho_y, color='r', label='sim')\n",
    "axes[2].legend()\n",
    "axes[2].set_xlabel('rt wrt stim')\n",
    "axes[2].set_ylabel('acc')\n",
    "axes[2].set_title('Grand Tacho')\n",
    "axes[2].set_ylim(0.5, 1);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
